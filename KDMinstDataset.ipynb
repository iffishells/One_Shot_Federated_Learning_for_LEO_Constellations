{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T08:11:54.424302Z",
     "start_time": "2025-12-22T08:11:53.770690Z"
    }
   },
   "source": "!pip install torch torchvision",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ifti/miniconda3/lib/python3.13/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: torchvision in /home/ifti/miniconda3/lib/python3.13/site-packages (0.24.0)\r\n",
      "Requirement already satisfied: filelock in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: setuptools in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (78.1.1)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (2.27.5)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (3.3.20)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.5.0 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torch) (3.5.0)\r\n",
      "Requirement already satisfied: numpy in /home/ifti/miniconda3/lib/python3.13/site-packages (from torchvision) (2.3.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ifti/miniconda3/lib/python3.13/site-packages (from torchvision) (12.0.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ifti/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ifti/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:11:58.999422Z",
     "start_time": "2025-12-22T08:11:58.402959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 0: Imports, device, seeds\n",
    "import math, random, copy, time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n"
   ],
   "id": "9595645c12d55362",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:12:16.364241Z",
     "start_time": "2025-12-22T08:12:08.200558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 1: MNIST loading (train as private; test as public proxy for KD)\n",
    "#   In LEOShot the server generates synthetic data (no public set).\n",
    "#   For this runnable demo, we use MNIST test images as the \"server distillation\" proxy.\n",
    "#   (A very common practice in FedMD-style KD demos.)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # [0,1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # standard MNIST normalization\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Train size:\", len(mnist_train), \"Test size:\", len(mnist_test))\n"
   ],
   "id": "3cb5c9dbcea08688",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.41MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 27.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000 Test size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:12:29.484769Z",
     "start_time": "2025-12-22T08:12:17.083561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 2: Build non-IID orbit splits for teachers\n",
    "# We'll create 5 orbits with different label sets.\n",
    "orbits_labels = [\n",
    "    [0,1,2],         # orbit 1\n",
    "    [3,4],           # orbit 2\n",
    "    [5,6],           # orbit 3\n",
    "    [7,8],           # orbit 4\n",
    "    [9,0,1]          # orbit 5 (overlaps to reflect heterogeneity)\n",
    "]\n",
    "\n",
    "def indices_for_labels(dataset, labels):\n",
    "    idxs = [i for i,(x,y) in enumerate(dataset) if y in labels]\n",
    "    return idxs\n",
    "\n",
    "orbit_train_subsets = []\n",
    "orbit_val_subsets   = []\n",
    "\n",
    "# Split per-orbit train into train/val (80/20)\n",
    "for lbls in orbits_labels:\n",
    "    idxs = indices_for_labels(mnist_train, lbls)\n",
    "    random.shuffle(idxs)\n",
    "    split = int(0.8 * len(idxs))\n",
    "    train_idxs, val_idxs = idxs[:split], idxs[split:]\n",
    "    orbit_train_subsets.append(Subset(mnist_train, train_idxs))\n",
    "    orbit_val_subsets.append(Subset(mnist_train, val_idxs))\n",
    "\n",
    "for i,lbls in enumerate(orbits_labels, start=1):\n",
    "    print(f\"Orbit {i} labels {lbls}: train {len(orbit_train_subsets[i-1])}, val {len(orbit_val_subsets[i-1])}\")\n"
   ],
   "id": "82d13dc0a9317a73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbit 1 labels [0, 1, 2]: train 14898, val 3725\n",
      "Orbit 2 labels [3, 4]: train 9578, val 2395\n",
      "Orbit 3 labels [5, 6]: train 9071, val 2268\n",
      "Orbit 4 labels [7, 8]: train 9692, val 2424\n",
      "Orbit 5 labels [9, 0, 1]: train 14891, val 3723\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:12:29.594400Z",
     "start_time": "2025-12-22T08:12:29.589476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 3: Define heterogeneous teacher CNNs and a smaller student\n",
    "# Teachers vary widths/depths to simulate heterogeneous architectures.\n",
    "class TeacherA(nn.Module):\n",
    "    def __init__(self, width=32, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, width, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, width*2, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(width*2)\n",
    "        self.fc    = nn.Linear(7*7*width*2, n_classes)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))),2)\n",
    "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))),2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class TeacherB(nn.Module):\n",
    "    def __init__(self, width=24, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, width, 5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width*2, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(width*2)\n",
    "        self.fc    = nn.Linear(7*7*width*2, n_classes)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))),2)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class StudentSmall(nn.Module):\n",
    "    def __init__(self, width=16, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, width, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(width)\n",
    "        self.fc    = nn.Linear(7*7*width, n_classes)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))),2)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ],
   "id": "d221f88611c72ce",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:12:53.777910Z",
     "start_time": "2025-12-22T08:12:30.724022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 4: Train one teacher\n",
    "def train_teacher(model, train_subset, val_subset, epochs=3, lr=1e-3, bs=128):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
    "    val_loader   = DataLoader(val_subset, batch_size=bs)\n",
    "    for ep in range(epochs):\n",
    "        model.train(); total=0; correct=0; loss_sum=0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss   = F.cross_entropy(logits, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            loss_sum += loss.item()*x.size(0)\n",
    "            pred = logits.argmax(1); correct += (pred==y).sum().item(); total += x.size(0)\n",
    "        # quick val\n",
    "        model.eval(); vtotal=0; vcorrect=0\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                out = model(x)\n",
    "                vcorrect += (out.argmax(1)==y).sum().item(); vtotal += x.size(0)\n",
    "        print(f\"Epoch {ep+1}: train loss {loss_sum/total:.3f}, train acc {correct/total:.3f}, val acc {vcorrect/vtotal:.3f}\")\n",
    "    return model.eval()\n",
    "\n",
    "# Train 5 teachers with heterogeneous architectures\n",
    "teachers = []\n",
    "for i,(tr,va) in enumerate(zip(orbit_train_subsets, orbit_val_subsets), start=1):\n",
    "    print(f\"\\nTraining Teacher for Orbit {i}, labels {orbits_labels[i-1]}\")\n",
    "    model = TeacherA(width=32) if i%2==1 else TeacherB(width=24)\n",
    "    t = train_teacher(model, tr, va, epochs=3, lr=1e-3, bs=128)\n",
    "    teachers.append(t)\n"
   ],
   "id": "ac8d90521c6eb253",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Teacher for Orbit 1, labels [0, 1, 2]\n",
      "Epoch 1: train loss 0.073, train acc 0.978, val acc 0.993\n",
      "Epoch 2: train loss 0.012, train acc 0.996, val acc 0.995\n",
      "Epoch 3: train loss 0.007, train acc 0.998, val acc 0.995\n",
      "\n",
      "Training Teacher for Orbit 2, labels [3, 4]\n",
      "Epoch 1: train loss 0.073, train acc 0.974, val acc 1.000\n",
      "Epoch 2: train loss 0.009, train acc 0.997, val acc 0.999\n",
      "Epoch 3: train loss 0.005, train acc 0.999, val acc 1.000\n",
      "\n",
      "Training Teacher for Orbit 3, labels [5, 6]\n",
      "Epoch 1: train loss 0.078, train acc 0.976, val acc 0.994\n",
      "Epoch 2: train loss 0.012, train acc 0.996, val acc 0.996\n",
      "Epoch 3: train loss 0.006, train acc 0.999, val acc 0.997\n",
      "\n",
      "Training Teacher for Orbit 4, labels [7, 8]\n",
      "Epoch 1: train loss 0.110, train acc 0.955, val acc 0.993\n",
      "Epoch 2: train loss 0.012, train acc 0.997, val acc 0.995\n",
      "Epoch 3: train loss 0.009, train acc 0.997, val acc 0.997\n",
      "\n",
      "Training Teacher for Orbit 5, labels [9, 0, 1]\n",
      "Epoch 1: train loss 0.063, train acc 0.981, val acc 0.995\n",
      "Epoch 2: train loss 0.009, train acc 0.998, val acc 0.996\n",
      "Epoch 3: train loss 0.006, train acc 0.998, val acc 0.994\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:13:02.324590Z",
     "start_time": "2025-12-22T08:13:02.322509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 5: Build the server \"distillation dataset\"\n",
    "# We'll use MNIST TEST split as the proxy that the server can query teachers on.\n",
    "kd_loader = DataLoader(mnist_test, batch_size=256, shuffle=True)\n",
    "\n",
    "# A held-out subset of TEST for eval (we'll keep it simple: reuse test for eval)\n",
    "eval_loader = DataLoader(mnist_test, batch_size=256)\n",
    "\n"
   ],
   "id": "790462f90ccdf376",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:17:31.870952Z",
     "start_time": "2025-12-22T08:14:02.150891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 6: Phase 2 KD — student learns teacher ensemble consensus (Eq. 16–17)\n",
    "@dataclass\n",
    "class KDConfig:\n",
    "    T: float = 4.0          # temperature\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 8\n",
    "    clip_grad: float = 1.0\n",
    "\n",
    "def kd_train(student:nn.Module, teachers:List[nn.Module], kd_loader, eval_loader, cfg:KDConfig):\n",
    "    student = student.to(device)\n",
    "    opt = torch.optim.Adam(student.parameters(), lr=cfg.lr)\n",
    "    for ep in range(cfg.epochs):\n",
    "        student.train(); loss_sum=0; total=0\n",
    "        for x,y in kd_loader:\n",
    "            x = x.to(device)   # NOTE: y not used in KD (we match teachers, not true labels)\n",
    "            with torch.no_grad():\n",
    "                # teacher ensemble logits (mean)\n",
    "                t_logits = [m(x) for m in teachers]\n",
    "                D_teacher = torch.stack(t_logits).mean(dim=0)      # [B,10]\n",
    "            # student logits\n",
    "            D_student = student(x)\n",
    "            # temperature-softened distributions\n",
    "            p = F.softmax(D_teacher / cfg.T, dim=1)                # teacher probs\n",
    "            q_log = F.log_softmax(D_student / cfg.T, dim=1)        # student log-probs\n",
    "            # KL(p || q) * T^2 (common KD scaling)\n",
    "            loss = F.kl_div(q_log, p, reduction='batchmean') * (cfg.T*cfg.T)\n",
    "            opt.zero_grad(); loss.backward()\n",
    "            if cfg.clip_grad: nn.utils.clip_grad_norm_(student.parameters(), cfg.clip_grad)\n",
    "            opt.step()\n",
    "            loss_sum += loss.item()*x.size(0); total += x.size(0)\n",
    "        # quick top-1 on eval\n",
    "        student.eval()\n",
    "        etotal=0\n",
    "        ecorrect=0\n",
    "        with torch.no_grad():\n",
    "            for x,y in eval_loader:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                out = student(x)\n",
    "                ecorrect += (out.argmax(1)==y).sum().item(); etotal += x.size(0)\n",
    "        print(f\"KD Epoch {ep+1}: train KL {loss_sum/total:.4f}, eval acc {ecorrect/etotal:.3f}\")\n",
    "    return student.eval()\n",
    "\n",
    "student = StudentSmall(width=16)\n",
    "kd_cfg  = KDConfig(T=4.0, lr=1e-3, epochs=100)\n",
    "student = kd_train(student, teachers, kd_loader, eval_loader, kd_cfg)\n"
   ],
   "id": "f4d3d60be1fe8159",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch 1: train KL 0.3885, eval acc 0.211\n",
      "KD Epoch 2: train KL 0.0480, eval acc 0.211\n",
      "KD Epoch 3: train KL 0.0316, eval acc 0.211\n",
      "KD Epoch 4: train KL 0.0244, eval acc 0.211\n",
      "KD Epoch 5: train KL 0.0219, eval acc 0.211\n",
      "KD Epoch 6: train KL 0.0197, eval acc 0.211\n",
      "KD Epoch 7: train KL 0.0176, eval acc 0.211\n",
      "KD Epoch 8: train KL 0.0169, eval acc 0.211\n",
      "KD Epoch 9: train KL 0.0156, eval acc 0.211\n",
      "KD Epoch 10: train KL 0.0146, eval acc 0.211\n",
      "KD Epoch 11: train KL 0.0146, eval acc 0.211\n",
      "KD Epoch 12: train KL 0.0138, eval acc 0.211\n",
      "KD Epoch 13: train KL 0.0149, eval acc 0.211\n",
      "KD Epoch 14: train KL 0.0128, eval acc 0.211\n",
      "KD Epoch 15: train KL 0.0122, eval acc 0.211\n",
      "KD Epoch 16: train KL 0.0125, eval acc 0.211\n",
      "KD Epoch 17: train KL 0.0131, eval acc 0.211\n",
      "KD Epoch 18: train KL 0.0117, eval acc 0.211\n",
      "KD Epoch 19: train KL 0.0115, eval acc 0.211\n",
      "KD Epoch 20: train KL 0.0116, eval acc 0.211\n",
      "KD Epoch 21: train KL 0.0110, eval acc 0.211\n",
      "KD Epoch 22: train KL 0.0112, eval acc 0.211\n",
      "KD Epoch 23: train KL 0.0111, eval acc 0.211\n",
      "KD Epoch 24: train KL 0.0102, eval acc 0.211\n",
      "KD Epoch 25: train KL 0.0108, eval acc 0.211\n",
      "KD Epoch 26: train KL 0.0107, eval acc 0.211\n",
      "KD Epoch 27: train KL 0.0107, eval acc 0.211\n",
      "KD Epoch 28: train KL 0.0094, eval acc 0.211\n",
      "KD Epoch 29: train KL 0.0094, eval acc 0.211\n",
      "KD Epoch 30: train KL 0.0095, eval acc 0.211\n",
      "KD Epoch 31: train KL 0.0095, eval acc 0.211\n",
      "KD Epoch 32: train KL 0.0091, eval acc 0.211\n",
      "KD Epoch 33: train KL 0.0101, eval acc 0.211\n",
      "KD Epoch 34: train KL 0.0095, eval acc 0.211\n",
      "KD Epoch 35: train KL 0.0107, eval acc 0.211\n",
      "KD Epoch 36: train KL 0.0095, eval acc 0.211\n",
      "KD Epoch 37: train KL 0.0093, eval acc 0.211\n",
      "KD Epoch 38: train KL 0.0092, eval acc 0.211\n",
      "KD Epoch 39: train KL 0.0088, eval acc 0.211\n",
      "KD Epoch 40: train KL 0.0085, eval acc 0.211\n",
      "KD Epoch 41: train KL 0.0092, eval acc 0.211\n",
      "KD Epoch 42: train KL 0.0087, eval acc 0.211\n",
      "KD Epoch 43: train KL 0.0086, eval acc 0.211\n",
      "KD Epoch 44: train KL 0.0084, eval acc 0.211\n",
      "KD Epoch 45: train KL 0.0084, eval acc 0.211\n",
      "KD Epoch 46: train KL 0.0086, eval acc 0.211\n",
      "KD Epoch 47: train KL 0.0092, eval acc 0.211\n",
      "KD Epoch 48: train KL 0.0092, eval acc 0.211\n",
      "KD Epoch 49: train KL 0.0096, eval acc 0.211\n",
      "KD Epoch 50: train KL 0.0085, eval acc 0.211\n",
      "KD Epoch 51: train KL 0.0082, eval acc 0.211\n",
      "KD Epoch 52: train KL 0.0082, eval acc 0.211\n",
      "KD Epoch 53: train KL 0.0085, eval acc 0.211\n",
      "KD Epoch 54: train KL 0.0086, eval acc 0.211\n",
      "KD Epoch 55: train KL 0.0083, eval acc 0.211\n",
      "KD Epoch 56: train KL 0.0081, eval acc 0.211\n",
      "KD Epoch 57: train KL 0.0103, eval acc 0.211\n",
      "KD Epoch 58: train KL 0.0081, eval acc 0.211\n",
      "KD Epoch 59: train KL 0.0083, eval acc 0.211\n",
      "KD Epoch 60: train KL 0.0077, eval acc 0.211\n",
      "KD Epoch 61: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 62: train KL 0.0084, eval acc 0.211\n",
      "KD Epoch 63: train KL 0.0084, eval acc 0.211\n",
      "KD Epoch 64: train KL 0.0075, eval acc 0.211\n",
      "KD Epoch 65: train KL 0.0085, eval acc 0.211\n",
      "KD Epoch 66: train KL 0.0078, eval acc 0.211\n",
      "KD Epoch 67: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 68: train KL 0.0082, eval acc 0.211\n",
      "KD Epoch 69: train KL 0.0088, eval acc 0.211\n",
      "KD Epoch 70: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 71: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 72: train KL 0.0081, eval acc 0.211\n",
      "KD Epoch 73: train KL 0.0078, eval acc 0.211\n",
      "KD Epoch 74: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 75: train KL 0.0078, eval acc 0.211\n",
      "KD Epoch 76: train KL 0.0072, eval acc 0.211\n",
      "KD Epoch 77: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 78: train KL 0.0077, eval acc 0.211\n",
      "KD Epoch 79: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 80: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 81: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 82: train KL 0.0078, eval acc 0.211\n",
      "KD Epoch 83: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 84: train KL 0.0078, eval acc 0.211\n",
      "KD Epoch 85: train KL 0.0078, eval acc 0.211\n",
      "KD Epoch 86: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 87: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 88: train KL 0.0074, eval acc 0.211\n",
      "KD Epoch 89: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 90: train KL 0.0073, eval acc 0.211\n",
      "KD Epoch 91: train KL 0.0081, eval acc 0.211\n",
      "KD Epoch 92: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 93: train KL 0.0071, eval acc 0.211\n",
      "KD Epoch 94: train KL 0.0075, eval acc 0.211\n",
      "KD Epoch 95: train KL 0.0076, eval acc 0.211\n",
      "KD Epoch 96: train KL 0.0070, eval acc 0.211\n",
      "KD Epoch 97: train KL 0.0075, eval acc 0.211\n",
      "KD Epoch 98: train KL 0.0070, eval acc 0.211\n",
      "KD Epoch 99: train KL 0.0072, eval acc 0.211\n",
      "KD Epoch 100: train KL 0.0075, eval acc 0.211\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:17:36.313393Z",
     "start_time": "2025-12-22T08:17:35.752167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 7: Final evaluation\n",
    "student.eval()\n",
    "total=0\n",
    "correct=0\n",
    "with torch.no_grad():\n",
    "    for x,y in eval_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        out = student(x)\n",
    "        correct += (out.argmax(1)==y).sum().item(); total += x.size(0)\n",
    "print(f\"Final Student Accuracy on MNIST test: {correct/total:.3f}\")\n"
   ],
   "id": "71fe0f48b22d16f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Student Accuracy on MNIST test: 0.211\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) (Optional) Server‑local virtual retraining (Phase 3 idea)\n",
    "This simulates LEOShot’s Phase 3 to further improve accuracy without any new satellite communication:\n",
    "clone several virtual students, partition the proxy data per orbit labels, train locally, then average."
   ],
   "id": "c74d7b43240817a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:29:19.871894Z",
     "start_time": "2025-12-22T08:18:45.211748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 8 (Optional): Partition MNIST test by orbit label sets\n",
    "def subset_by_labels(dataset, labels, max_items=None):\n",
    "    idxs = [i for i,(x,y) in enumerate(dataset) if y in labels]\n",
    "    if max_items: idxs = idxs[:max_items]\n",
    "    return Subset(dataset, idxs)\n",
    "\n",
    "parts = [ subset_by_labels(mnist_test, lbls, max_items=400) for lbls in orbits_labels ]\n",
    "\n",
    "def train_virtual(model_init, ds, epochs=2, lr=5e-4, bs=128):\n",
    "    m = copy.deepcopy(model_init).to(device)\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n",
    "    for _ in range(epochs):\n",
    "        m.train()\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            loss = F.cross_entropy(m(x), y)   # standard supervised loss\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return m.eval()\n",
    "\n",
    "def avg_state_dicts(dicts):\n",
    "    avg={}\n",
    "    for k in dicts[0].keys():\n",
    "        avg[k] = sum(d[k] for d in dicts) / len(dicts)\n",
    "    return avg\n",
    "\n",
    "rounds=50\n",
    "for r in range(rounds):\n",
    "    v_states=[]\n",
    "    for ds in parts:\n",
    "        vm = train_virtual(student, ds, epochs=100, lr=5e-4, bs=128)\n",
    "        v_states.append(vm.state_dict())\n",
    "    student.load_state_dict(avg_state_dicts(v_states))\n",
    "\n",
    "# Re-evaluate\n",
    "student.eval(); total=0; correct=0\n",
    "with torch.no_grad():\n",
    "    for x,y in eval_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        out = student(x)\n",
    "        correct += (out.argmax(1)==y).sum().item(); total += x.size(0)\n",
    "print(f\"After Virtual Retraining: Student Accuracy: {correct/total:.3f}\")\n"
   ],
   "id": "51c8ae131fbdda57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Virtual Retraining: Student Accuracy: 0.780\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "237d0e4b6a6558b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
